{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, neighbors, svm\n",
    "from sklearn.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhension</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  clump_thickness  unif_cell_size  unif_cell_shape  marg_adhension  \\\n",
       "0  1000025                5               1                1               1   \n",
       "1  1002945                5               4                4               5   \n",
       "2  1015425                3               1                1               1   \n",
       "3  1016277                6               8                8               1   \n",
       "4  1017023                4               1                1               3   \n",
       "\n",
       "   single_epith_cell_size bare_nuclei  bland_chromatin  norm_nucleoli  \\\n",
       "0                       2           1                3              1   \n",
       "1                       7          10                3              2   \n",
       "2                       2           2                3              1   \n",
       "3                       3           4                3              7   \n",
       "4                       2           1                3              1   \n",
       "\n",
       "   mitoses  class  \n",
       "0        1      2  \n",
       "1        1      2  \n",
       "2        1      2  \n",
       "3        1      2  \n",
       "4        1      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing data\n",
    "\n",
    "df.replace('?', -99999, inplace = True)\n",
    "#most algorithms recognize (-99999) as a n outlier and would treat it as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y\n",
    "#X = features\n",
    "#y = labels\n",
    "\n",
    "X = np.array(df.drop(['class'], 1))\n",
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing our cross validation, splitting our data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining our classifier\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "#Testing our data\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a prediction\n",
    "\n",
    "example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 15\n",
    "\n",
    "Euclidean Distance\n",
    "\n",
    "Euclidean distance is the distance between two points defined as the square root of the sum of the squares of the differences between the corresponding coordinates of the points.\n",
    "\n",
    "Where n is the number of dimensions in the dataset\n",
    "\n",
    "i is dimensions, p is 1 point and q is another point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "plot1 = [1,3]\n",
    "plot2 = [2,5]\n",
    "\n",
    "euclidean_distance = sqrt ((plot1[0] - plot2[0])**2 + (plot1[1] - plot2[1])**2)\n",
    "\n",
    "print (euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 16\n",
    "\n",
    "Writing our K nearest neighbours algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "import warnings\n",
    "from collections import Counter\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'k':[[1,2],[2,3],[3,1]], 'r': [[6,5],[7,7],[8,6]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = [5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dataset:\n",
    "#     for ii in dataset [i]:\n",
    "#         plt.scatter(ii[0], ii[1], s = 100, color = i)\n",
    "        \n",
    "#you can also write the above equation as[[plt.scatter(ii[0], ii[1], s = 100, color = i)for ii indataset[i]]for i in dataset]\n",
    "\n",
    "# plt.scatter(new_features[0], new_features[1])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "euclidean_distance = sqrt((feature[0]-prediction[0]) ** 2 + (feature[1]-prediction[1]) ** 2)\n",
    "\n",
    "the above equation for euclidean distance would work when it is a 2 feauture dimensional dataset, but would not work when it is more than a 2 feature dimensions dataset\n",
    "\n",
    "So we use this instead\n",
    "\n",
    "euclidean_distance = np.sqrt(np.sum((np.array(features) - np.array(predict)) ** 2 )\n",
    "\n",
    "The simpler version of this is:\n",
    "\n",
    "euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r', 3)]\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "#defining the k nearest neighbour algorithm\n",
    "\n",
    "def k_nearest_neighbor(data, predict, k = 3):\n",
    "    if len(data) >= k:\n",
    "        warnings.warn('K is set to a value less than total warning groups!')\n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distances.append([euclidean_distance, group])\n",
    "    votes = [i[1]for i in sorted (distances)[:k]]\n",
    "    #print(Counter(votes).most_common(1))\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    \n",
    "    \n",
    "#     knnalgos\n",
    "    return vote_result\n",
    "\n",
    "result = k_nearest_neighbor(dataset, new_features, k = 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Objective is to manually create our KNN algorithm and to  do this we would require three arguments ( the data we want to train,  the prediction we want to make and the number of neighbors we want to use) .We need to first understand some basics, such as\n",
    "\n",
    "1. KNN classifies by choosing the closest data point to our predicting value using Euclidean distance.  The Euclidean distance measures the difference in length between the predicting value and every datapoint (features)\n",
    "2. The number of neighbors(k) should be greater than the number of classes for instance if I have 2 classes  and I set my K to be 1 . My prediction can have the same Euclidean distance between both classes hence we won't be able to vote  for a class, else if I have k=3 my prediction will definitely have at least two Euclidean distance to a class ,thereby choosing this class .\n",
    "\n",
    "Going to the code a function is defined with the the basics we will need ( data, prediction,  and neighbors)=> def K_nearest_neigbhors.\n",
    "\n",
    "If len(data)>=K it will flag a warning i.e the number of class should be less than K.\n",
    "For the\" for loop\"  our data here is in form of a dictionary, hence to access the features (dictionary values)  we need a double for loop the first to access the class/group/dictionary keys (for group in data)  second to access the features ( for features in data[group]) .\n",
    "\n",
    "For each feature there exist a distance (Euclidean distance)  that measures the difference between each features and the new prediction , which is then passed into a list alongside the class/group of the feature ( distance.append(E.D, group)\n",
    "\n",
    "Since we are only interested in the 3 nearest neighbor we first sort the distance in ascending order and stop at the 3rd index (sorted (distance) [:3].\n",
    "The resulting list is then iterated over to check the the class with the highest number of votes ..votes =[i[1] for In  the sorted distance...\n",
    "\n",
    "*note that the list is a list of lists with two elements in the each sublist that contains the distance in the 0th position and class in the 1st position * that's why we are iterating over i[1] not just i or i[0] because we are really interested in the class not the distance\n",
    "Counter (votes).  Most_common(1) => The counter function counts the number of votes for most common class(1 represents classes)\n",
    "Vote_result = counter(votes).most_common(1)[0][0]=> since the result gives a list of tuple , where the first element in the tuple is the class and the second element represents the number of votes of the class to access the first tuple and first element in the tuple which represent the class we have [0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 18\n",
    "\n",
    "Using our breast cancer dataset to have a broader understanding of k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbor(data, predict, k = 3):\n",
    "    if len(data) >= k:\n",
    "        warnings.warn('K is set to a value less than total warning groups!')\n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distances.append([euclidean_distance, group])\n",
    "    votes = [i[1]for i in sorted (distances)[:k]]\n",
    "    #print(Counter(votes).most_common(1))\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    confidence = Counter(votes).most_common(1)[0][1] / k\n",
    "    \n",
    "    #print(vote_result, confidence)\n",
    "#     knnalgos\n",
    "    return vote_result, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?', -99999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #converting the contents of the dataset to float\n",
    "\n",
    "full_data = df.astype(float).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #shuffling the data so as not to lose the features of the data (this is like scaling)\n",
    "\n",
    "random.shuffle(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a version of train_test_split \n",
    "\n",
    "test_size = 0.4\n",
    "train_set = {2:[], 4:[]}\n",
    "test_set = {2:[], 4:[]}\n",
    "\n",
    "#slicing the data\n",
    "train_data = full_data [:-int(test_size*len(full_data))]\n",
    "test_data = full_data [-int(test_size*len(full_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populating the empty dictionaries\n",
    "\n",
    "for i in train_data:\n",
    "    train_set [i[-1]].append(i[:-1])\n",
    "    \n",
    "for i in test_data:\n",
    "    test_set [i[-1]].append(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9605734767025089\n"
     ]
    }
   ],
   "source": [
    "#passing the information through to k nearest neighbors\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for group in test_set:\n",
    "    for data in test_set[group]:\n",
    "        vote, confidence = k_nearest_neighbor(train_set, data, k=5)\n",
    "        if group == vote:\n",
    "            correct += 1\n",
    "#             else:\n",
    "#                 print(confidence)\n",
    "        total += 1\n",
    "\n",
    "print ('Accuracy:', correct/total)\n",
    "accuracies.append(correct/total)\n",
    "\n",
    "# print(sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 19\n",
    "\n",
    "K Accuracy and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing K does not necessarily do you a favour\n",
    "\n",
    "**Confidence vs Accuracy**\n",
    "\n",
    "Accuracy - Did we get the classification right?\n",
    "\n",
    "Confidence can come from the classifier\n",
    "\n",
    "The ratio of the voter result to the value of K is known as the Confidence interval.\n",
    "\n",
    "When the test size is increased the confidence decreases.\n",
    "\n",
    "**Some facts about k nearest neighbors**\n",
    "- k nearest neighbors can be threaded, so you don't have to test each prediction point linearly, you can test each one on their own\n",
    "- KNN can work on both linear and non linear data.\n",
    "\n",
    "For linear data, you use regression for classification\n",
    "\n",
    "For non linear data, you can't do classification, but you can do K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 20\n",
    "\n",
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is another supervised machine learning classifier. It is the most popular machine learning algorithm.\n",
    "\n",
    "SVM is a binary classifier so it separates only into 2 groups at a time. The 2 groups are denoted as positive and negative.\n",
    "\n",
    "The objective of SVM is to find the best separating hyper plane or decision boundary that will separate data.\n",
    "\n",
    "When you get the best separating hyper plane, you can now take in unknown data, if the unknown data rests on the positive side of the hyperplane, it becomes positive sample, and if it rests on the negative side, it becomes negative.\n",
    "\n",
    "So the intuition of SVM is to find the best separating hyperplane and then we can classify new datapoints.\n",
    "\n",
    "The goal of the SVM algorithm is to find the shortest distance to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing data\n",
    "\n",
    "df.replace('?', -99999, inplace = True)\n",
    "#most algorithms recognize (-99999) as a n outlier and would treat it as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y\n",
    "#X = features\n",
    "#y = labels\n",
    "\n",
    "X = np.array(df.drop(['class'], 1))\n",
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing our cross validation, splitting our data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chidi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining our classifier\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "#Testing our data\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a prediction\n",
    "\n",
    "example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Vectors**\n",
    "\n",
    "A vector has both magnitude and direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 22\n",
    "\n",
    "**Support Vector Assertion**\n",
    "\n",
    "SVM creates a decision boundary, the way a SVM classifies new points once it reaches the decision bounday is by first taking the vector that point perpendicularly to the separating hyperplane(vector w), you would project vector u(unknown data plane) on to vector w, then you would find out what side of the hyper plane vector u is on.\n",
    "\n",
    "What is the calculation once we've trained a machine learning classifier?\n",
    "\n",
    "vector u * vector w + b(bias)\n",
    "\n",
    "If the equation above is >= 0, then it is a positive sample.\n",
    "\n",
    "If the equation Vector u * vector w + b <= 0, then it is a negative sample.\n",
    "\n",
    "If vector u * vector w = 0, then it means that it is on the decision boundary.\n",
    "\n",
    "The unknown, vector u is a feature set comprised of x1 and x2\n",
    "\n",
    "**How can we make an equation to go through our data and locate support vectors?\n",
    "\n",
    "We introduce Y(subscript i) - this is the class of the features that we are passing through.\n",
    "\n",
    "If the class is a + class, then Y (sub i) = +1 or 1\n",
    "\n",
    "If the class is a - class, then Y (sub i) = -1 \n",
    "\n",
    "We now multiply Y(sub i) by the equaations we were using to identify the positive and negative support vectors.\n",
    "\n",
    "+class --> Xi * vector w + b = 1\n",
    "\n",
    "-class --> Xi * vector w + b = -1\n",
    "\n",
    "So now we multiply the equations above by Y(sub i) and then we set both equations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
