{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, neighbors, svm\n",
    "from sklearn.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhension</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  clump_thickness  unif_cell_size  unif_cell_shape  marg_adhension  \\\n",
       "0  1000025                5               1                1               1   \n",
       "1  1002945                5               4                4               5   \n",
       "2  1015425                3               1                1               1   \n",
       "3  1016277                6               8                8               1   \n",
       "4  1017023                4               1                1               3   \n",
       "\n",
       "   single_epith_cell_size bare_nuclei  bland_chromatin  norm_nucleoli  \\\n",
       "0                       2           1                3              1   \n",
       "1                       7          10                3              2   \n",
       "2                       2           2                3              1   \n",
       "3                       3           4                3              7   \n",
       "4                       2           1                3              1   \n",
       "\n",
       "   mitoses  class  \n",
       "0        1      2  \n",
       "1        1      2  \n",
       "2        1      2  \n",
       "3        1      2  \n",
       "4        1      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing data\n",
    "\n",
    "df.replace('?', -99999, inplace = True)\n",
    "#most algorithms recognize (-99999) as a n outlier and would treat it as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y\n",
    "#X = features\n",
    "#y = labels\n",
    "\n",
    "X = np.array(df.drop(['class'], 1))\n",
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing our cross validation, splitting our data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining our classifier\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "#Testing our data\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a prediction\n",
    "\n",
    "example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 15\n",
    "\n",
    "Euclidean Distance\n",
    "\n",
    "Euclidean distance is the distance between two points defined as the square root of the sum of the squares of the differences between the corresponding coordinates of the points.\n",
    "\n",
    "Where n is the number of dimensions in the dataset\n",
    "\n",
    "i is dimensions, p is 1 point and q is another point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "plot1 = [1,3]\n",
    "plot2 = [2,5]\n",
    "\n",
    "euclidean_distance = sqrt ((plot1[0] - plot2[0])**2 + (plot1[1] - plot2[1])**2)\n",
    "\n",
    "print (euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 16\n",
    "\n",
    "Writing our K nearest neighbours algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "import warnings\n",
    "from collections import Counter\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'k':[[1,2],[2,3],[3,1]], 'r': [[6,5],[7,7],[8,6]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = [5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dataset:\n",
    "#     for ii in dataset [i]:\n",
    "#         plt.scatter(ii[0], ii[1], s = 100, color = i)\n",
    "        \n",
    "#you can also write the above equation as[[plt.scatter(ii[0], ii[1], s = 100, color = i)for ii indataset[i]]for i in dataset]\n",
    "\n",
    "# plt.scatter(new_features[0], new_features[1])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "euclidean_distance = sqrt((feature[0]-prediction[0]) ** 2 + (feature[1]-prediction[1]) ** 2)\n",
    "\n",
    "the above equation for euclidean distance would work when it is a 2 feauture dimensional dataset, but would not work when it is more than a 2 feature dimensions dataset\n",
    "\n",
    "So we use this instead\n",
    "\n",
    "euclidean_distance = np.sqrt(np.sum((np.array(features) - np.array(predict)) ** 2 )\n",
    "\n",
    "The simpler version of this is:\n",
    "\n",
    "euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "#defining the k nearest neighbour algorithm\n",
    "\n",
    "def k_nearest_neighbor(data, predict, k = 3):\n",
    "    if len(data) >= k:\n",
    "        warnings.warn('K is set to a value less than total warning groups!')\n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distances.append([euclidean_distance, group])\n",
    "    votes = [i[1]for i in sorted (distances)[:k]]\n",
    "    #print(Counter(votes).most_common(1))\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    \n",
    "    \n",
    "#     knnalgos\n",
    "    return vote_result\n",
    "\n",
    "result = k_nearest_neighbor(dataset, new_features, k = 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Objective is to manually create our KNN algorithm and to  do this we would require three arguments ( the data we want to train,  the prediction we want to make and the number of neighbors we want to use) .We need to first understand some basics, such as\n",
    "\n",
    "1. KNN classifies by choosing the closest data point to our predicting value using Euclidean distance.  The Euclidean distance measures the difference in length between the predicting value and every datapoint (features)\n",
    "2. The number of neighbors(k) should be greater than the number of classes for instance if I have 2 classes  and I set my K to be 1 . My prediction can have the same Euclidean distance between both classes hence we won't be able to vote  for a class, else if I have k=3 my prediction will definitely have at least two Euclidean distance to a class ,thereby choosing this class .\n",
    "\n",
    "Going to the code a function is defined with the the basics we will need ( data, prediction,  and neighbors)=> def K_nearest_neigbhors.\n",
    "\n",
    "If len(data)>=K it will flag a warning i.e the number of class should be less than K.\n",
    "For the\" for loop\"  our data here is in form of a dictionary, hence to access the features (dictionary values)  we need a double for loop the first to access the class/group/dictionary keys (for group in data)  second to access the features ( for features in data[group]) .\n",
    "\n",
    "For each feature there exist a distance (Euclidean distance)  that measures the difference between each features and the new prediction , which is then passed into a list alongside the class/group of the feature ( distance.append(E.D, group)\n",
    "\n",
    "Since we are only interested in the 3 nearest neighbor we first sort the distance in ascending order and stop at the 3rd index (sorted (distance) [:3].\n",
    "The resulting list is then iterated over to check the the class with the highest number of votes ..votes =[i[1] for In  the sorted distance...\n",
    "\n",
    "*note that the list is a list of lists with two elements in the each sublist that contains the distance in the 0th position and class in the 1st position * that's why we are iterating over i[1] not just i or i[0] because we are really interested in the class not the distance\n",
    "Counter (votes).  Most_common(1) => The counter function counts the number of votes for most common class(1 represents classes)\n",
    "Vote_result = counter(votes).most_common(1)[0][0]=> since the result gives a list of tuple , where the first element in the tuple is the class and the second element represents the number of votes of the class to access the first tuple and first element in the tuple which represent the class we have [0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 18\n",
    "\n",
    "Using our breast cancer dataset to have a broader understanding of k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbor(data, predict, k = 3):\n",
    "    if len(data) >= k:\n",
    "        warnings.warn('K is set to a value less than total warning groups!')\n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distances.append([euclidean_distance, group])\n",
    "    votes = [i[1]for i in sorted (distances)[:k]]\n",
    "    #print(Counter(votes).most_common(1))\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    confidence = Counter(votes).most_common(1)[0][1] / k\n",
    "    \n",
    "    #print(vote_result, confidence)\n",
    "#     knnalgos\n",
    "    return vote_result, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?', -99999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #converting the contents of the dataset to float\n",
    "\n",
    "full_data = df.astype(float).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #shuffling the data so as not to lose the features of the data (this is like scaling)\n",
    "\n",
    "random.shuffle(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a version of train_test_split \n",
    "\n",
    "test_size = 0.4\n",
    "train_set = {2:[], 4:[]}\n",
    "test_set = {2:[], 4:[]}\n",
    "\n",
    "#slicing the data\n",
    "train_data = full_data [:-int(test_size*len(full_data))]\n",
    "test_data = full_data [-int(test_size*len(full_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populating the empty dictionaries\n",
    "\n",
    "for i in train_data:\n",
    "    train_set [i[-1]].append(i[:-1])\n",
    "    \n",
    "for i in test_data:\n",
    "    test_set [i[-1]].append(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "#passing the information through to k nearest neighbors\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for group in test_set:\n",
    "    for data in test_set[group]:\n",
    "        vote, confidence = k_nearest_neighbor(train_set, data, k=5)\n",
    "        if group == vote:\n",
    "            correct += 1\n",
    "#             else:\n",
    "#                 print(confidence)\n",
    "        total += 1\n",
    "\n",
    "print ('Accuracy:', correct/total)\n",
    "#accuracies.append(correct/total)\n",
    "\n",
    "# print(sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 19\n",
    "\n",
    "K Accuracy and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing K does not necessarily do you a favour\n",
    "\n",
    "**Confidence vs Accuracy**\n",
    "\n",
    "Accuracy - Did we get the classification right?\n",
    "\n",
    "Confidence can come from the classifier\n",
    "\n",
    "The ratio of the voter result to the value of K is known as the Confidence interval.\n",
    "\n",
    "When the test size is increased the confidence decreases.\n",
    "\n",
    "**Some facts about k nearest neighbors**\n",
    "- k nearest neighbors can be threaded, so you don't have to test each prediction point linearly, you can test each one on their own\n",
    "- KNN can work on both linear and non linear data.\n",
    "\n",
    "For linear data, you use regression for classification\n",
    "\n",
    "For non linear data, you can't do classification, but you can do K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 20\n",
    "\n",
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is another supervised machine learning classifier. It is the most popular machine learning algorithm.\n",
    "\n",
    "SVM is a binary classifier so it separates only into 2 groups at a time. The 2 groups are denoted as positive and negative.\n",
    "\n",
    "The objective of SVM is to find the best separating hyper plane or decision boundary that will separate data.\n",
    "\n",
    "When you get the best separating hyper plane, you can now take in unknown data, if the unknown data rests on the positive side of the hyperplane, it becomes positive sample, and if it rests on the negative side, it becomes negative.\n",
    "\n",
    "So the intuition of SVM is to find the best separating hyperplane and then we can classify new datapoints.\n",
    "\n",
    "The goal of the SVM algorithm is to find the shortest distance to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing data\n",
    "\n",
    "df.replace('?', -99999, inplace = True)\n",
    "#most algorithms recognize (-99999) as a n outlier and would treat it as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "\n",
    "df.drop(['id'], 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y\n",
    "#X = features\n",
    "#y = labels\n",
    "\n",
    "X = np.array(df.drop(['class'], 1))\n",
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing our cross validation, splitting our data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chidi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining our classifier\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9785714285714285\n"
     ]
    }
   ],
   "source": [
    "#Testing our data\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a prediction\n",
    "\n",
    "example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]])\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Vectors**\n",
    "\n",
    "A vector has both magnitude and direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 22\n",
    "\n",
    "**Support Vector Assertion**\n",
    "\n",
    "SVM creates a decision boundary, the way a SVM classifies new points once it reaches the decision bounday is by first taking the vector that point perpendicularly to the separating hyperplane(vector w), you would project vector u(unknown data plane) on to vector w, then you would find out what side of the hyper plane vector u is on.\n",
    "\n",
    "What is the calculation once we've trained a machine learning classifier?\n",
    "\n",
    "vector u * vector w + b(bias)\n",
    "\n",
    "If the equation above is >= 0, then it is a positive sample.\n",
    "\n",
    "If the equation Vector u * vector w + b <= 0, then it is a negative sample.\n",
    "\n",
    "If vector u * vector w = 0, then it means that it is on the decision boundary.\n",
    "\n",
    "The unknown, vector u is a feature set comprised of x1 and x2\n",
    "\n",
    "**How can we make an equation to go through our data and locate support vectors?\n",
    "\n",
    "We introduce Y(subscript i) - this is the class of the features that we are passing through.\n",
    "\n",
    "If the class is a + class, then Y (sub i) = +1 or 1\n",
    "\n",
    "If the class is a - class, then Y (sub i) = -1 \n",
    "\n",
    "We now multiply Y(sub i) by the equaations we were using to identify the positive and negative support vectors.\n",
    "\n",
    "+class --> Xi * vector w + b = 1\n",
    "\n",
    "-class --> Xi * vector w + b = -1\n",
    "\n",
    "So now we multiply the equations above by Y(sub i) and then we set both equations = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation for the width that separates the hyperplane that goes through the support vectors\n",
    "\n",
    "The support vector is a feature set that if moved, affects the position of the best separating hyperplane.\n",
    "\n",
    "The separating hyperplane can be calculated by taking the width divided by 2.***\n",
    "\n",
    "**Downside of SVM:** In SVM, you need all your featureset in memory to optimize, so if you have a very large dataset, it might not be feasible.\n",
    "\n",
    "One way to go around this is you can use sequential minimal optimization when working with large dataset.\n",
    "\n",
    "**Upside of SVM:** Once you have trained the SVM, you don't need old features anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine Optimization**\n",
    "\n",
    "Equation for hyperplane is X(sub i)*W+b\n",
    "\n",
    "Hyperplane for a positive class support vector => Xi*W+b = 1\n",
    "\n",
    "Hyperplane for a negative class support vector => Xi*W+b = -1\n",
    "\n",
    "Decision boundary => X*W+b=0 a feature set part of the decision bounday.\n",
    "\n",
    "Formular for classification of a feature set after we've trained our classifier and optimized for W and b -> \"sign\"(Xi*W+b).\n",
    "\n",
    "- \"Sign\" is positive if the above is above 0.\n",
    "- \"Sign\" is negative if the above is below 0.\n",
    "- \"Sign\" is a decision boundary if the above is 0.\n",
    "\n",
    "Optimization Objective:\n",
    "\n",
    "- To minimize the magnitude of vector W --> ||W||\n",
    "- To maximize the bias b\n",
    "\n",
    "What is the constraint of W and b when you plug it into an equation? --> Yi(Xi * W + b) >= 1 where Yi is the class, Xi is the known features, W is vector.\n",
    "\n",
    "You can say that Yi(Xi * W+b) >= 1 ==> class(known features*W + b) >= 1. The known features are the testing data.\n",
    "\n",
    "The shape of vector W is a 1 by 2 (1 x 2) matrix. Example [5, 3], the magnitude of vector is square root(5^2 + 3^2) which is the square root(34)\n",
    "\n",
    "\n",
    "if vector W is [5, 5], you would plug it into Yi(Xi*W+b) >= 1 to see if we would find any b (bias) that satisfies the equation.\n",
    "\n",
    "Then you would set it down to [4, 4], plug it into the equation to see if we would find any b(bias) that satisfies the equation, you would keep stepping it down until you can't step it down any more.\n",
    "\n",
    "if Vector W = [10, 10], you plug it into Yi(Xi* W+b) >= 1, you would keep passing it to see if you can find something that satisfies the equation, if you can, you would save it to a dictionary called mag {||W|| : [W * b]} then you would find the one that has the lowest key or magnitude that would be your answer.\n",
    "\n",
    "The minimum point is called the global minimum.\n",
    "\n",
    "When you plug in every feature into the equation Yi(Xi * W + b) >= 1, the support vector would equal 1.\n",
    "\n",
    "How will you know when you have solved the problem?\n",
    "\n",
    "You would have something very close to 1 both for your positive and negative feature set when you feed it through the algorithm Yi(Xi * W + b) >= 1.\n",
    "\n",
    "Support vector machine is a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 25 & Day 26 & Day 27 & Day 28\n",
    "\n",
    "**Creating an SVM from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized a step\n",
      "Optimized a step\n",
      "Optimized a step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Support_Vector_Machine:\n",
    "    def __init__ (self, visualization = True):  #visualization would help to 'visualize'\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1: 'r', -1:'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    #train\n",
    "    def fit(self, data):  \n",
    "        self.data = data\n",
    "        #{ ||w||:[w,b] }\n",
    "        opt_dict = {}\n",
    "        \n",
    "        transforms = [[1,1],     #the tranforms are applied to the vector of 'w' as we step each time\n",
    "                     [-1,1],\n",
    "                     [-1,-1],\n",
    "                     [1,-1]]\n",
    "        all_data = [] #we are doing this so that we would get our maximum and minimum range\n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feature in featureset:\n",
    "                    all_data.append(feature)\n",
    "                    \n",
    "        self.max_feature_value = max (all_data)\n",
    "        self.min_feature_value = min (all_data)\n",
    "        all_data = None\n",
    "        \n",
    "        step_sizes = [self.max_feature_value * 0.1,\n",
    "                     self.max_feature_value * 0.01,\n",
    "                      #point of expense:\n",
    "                     self.max_feature_value * 0.001,]\n",
    "        \n",
    "        #extremely expensive:\n",
    "        b_range_multiple = 2\n",
    "        #we don't need to take as small of steps with b as we do with w\n",
    "        b_multiple = 5\n",
    "        \n",
    "        latest_optimum = self.max_feature_value*10\n",
    "        \n",
    "        #the stepping process\n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum, latest_optimum])\n",
    "            \n",
    "            #we can do this because of convex optimization\n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                #we would iterate through b,   \n",
    "                #np.arange is like range but it allows us to say how much of a step to take per time\n",
    "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),self.max_feature_value*b_range_multiple,step*b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_t = w*transformation\n",
    "                        found_option = True\n",
    "#this is the weakest link in the SVM (this means that we have to run this function on all the data to make sure it fits)\n",
    "#SMO attempts to fix this a bit\n",
    "                        for i in self.data:\n",
    "                            for xi in self.data[i]:\n",
    "                                yi = i\n",
    "                                if not yi*(np.dot(w_t, xi)+b)>=1:\n",
    "                                    found_option = False\n",
    "                                #print(yi*(np.dot(w_t, xi)+b))\n",
    "                                \n",
    "                        if found_option:\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
    "                if w[0]<0:\n",
    "                    optimized = True\n",
    "                    print ('Optimized a step')\n",
    "                else:\n",
    "                    w = w - step\n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            # ||w|| : [w,b]\n",
    "            opt_choice = opt_dict [norms[0]]\n",
    "            \n",
    "            self.w = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0]+step*2     \n",
    "    \n",
    "    def predict(self, features):\n",
    "        # sign (X.W+b) |dot product returns a scalar value\n",
    "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
    "        if classification != 0 and self.visualization:\n",
    "            self.ax.scatter(features [0], features [1], s = 200, marker = '*', c = self.colors[classification])\n",
    "        return classification\n",
    "    \n",
    "    def visualize(self):\n",
    "        [[self.ax.scatter(x[0], x[1], s = 100, color = self.colors[i])for x in data_dict[i]]for i in data_dict]\n",
    "        \n",
    "        def hyperplane(x,w,b,v): #this function gives us the hyperplane values where v is the value we are seeking.\n",
    "            return (-w[0] * x - b + v) / w[1]\n",
    "        datarange = (self.min_feature_value * 0.9, self.max_feature_value * 1.1)#this is used to limit our graph\n",
    "        hyp_x_min = datarange[0]\n",
    "        hyp_x_max = datarange[1]\n",
    "        \n",
    "        #positive support vector hyperplane --> (w*x+b) = 1\n",
    "        psv1 = hyperplane(hyp_x_min, self.w, self.b, 1) #psv would be a scalar value. In a graph, psv would be 'y'\n",
    "        psv2 = hyperplane(hyp_x_max, self.w, self.b, 1)\n",
    "        self.ax.plot([hyp_x_min, hyp_x_max], [psv1, psv2], 'k') #where hyp_x_min would be the 'x' in our graph\n",
    "        #'k' means black color\n",
    "        \n",
    "        #negative support vector hyperplane --> (w*x+b) = -1\n",
    "        nsv1 = hyperplane(hyp_x_min, self.w, self.b, -1) #nsv would be a scalar value. In a graph, nsv would be 'y'\n",
    "        nsv2 = hyperplane(hyp_x_max, self.w, self.b, -1)\n",
    "        self.ax.plot([hyp_x_min, hyp_x_max], [nsv1, nsv2], 'k') #where hyp_x_min would be the 'x' in our graph\n",
    "        \n",
    "        #decision support boundary --> (w*x+b) = 0\n",
    "        db1 = hyperplane(hyp_x_min, self.w, self.b, 0) #db would be a scalar value. In a graph, db would be 'y'\n",
    "        db2 = hyperplane(hyp_x_max, self.w, self.b, 0)\n",
    "        self.ax.plot([hyp_x_min, hyp_x_max], [db1, db2], 'y--') #where hyp_x_min would be the 'x' in our graph\n",
    "        # 'y--' means yellow color\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "data_dict = {-1: np.array ([[1, 7],\n",
    "                           [2, 8],\n",
    "                           [3, 8],]),\n",
    "            1: np.array ([[5, 1],\n",
    "                          [6, -1],\n",
    "                          [7, 3],])}\n",
    "\n",
    "svm = Support_Vector_Machine()\n",
    "svm.fit(data = data_dict)\n",
    "\n",
    "predict_us = [[0, 10],\n",
    "             [1, 3],\n",
    "             [3, 4],\n",
    "             [3, 5],\n",
    "             [5, 5],\n",
    "             [5, 6],\n",
    "             [6, -5],\n",
    "             [5, 8]]\n",
    "\n",
    "for p in predict_us:\n",
    "    svm.predict(p)\n",
    "    \n",
    "svm.visualize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the larger your dataset, the longer it's going to take to do the fitment process, because you have to check every variable to see if it soots your equation.\n",
    "\n",
    "convex optimization is valuable because you can tell when point of optimization is reached\n",
    "\n",
    "we would iterate through b - not only do we want to check the magnitude of vector w, we also want the maximum b (most bias possible)\n",
    "\n",
    "we have step* b_multiple because we don't need to take as small of steps as we have with w\n",
    "\n",
    "You would know that you have found great values of the weights and bias(w and b) given that SVM is yi(xi. w+b) = 1 when if both your positive and negative classes, you have a value that is relatively close to 1 and until, you hit that value, you keep stepping.\n",
    "\n",
    "Although, you should know that some problems can't be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want v = x.w+b\n",
    "\n",
    "Examples where we care what v is:\n",
    "\n",
    "- in the positive support vector v = 1\n",
    "- in the negative support vector, v = -1\n",
    "- in the decision boundary, v = 0\n",
    "\n",
    "It's important to note that the visualize function and the hyperplane function have no bearing on the SVM at all, it is purely for visualization for humans. It is mostly matplotlib stuff going on there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 29\n",
    "\n",
    "**Introduction to Kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use kernels to do calculations in plausible infinite dimensions without having to pay the processing cost.\n",
    "\n",
    "A kernel is a similarity function, it takes 2 inputs and outputs the similarity of these inputs.\n",
    "\n",
    "It allows us to work with non-linear data by transforming the non linear data to another dimension and creating a linearly separable situation.\n",
    "\n",
    "One major element to Kernels is that they are done using inner product.\n",
    "\n",
    "Inner product and dot product are the same thing.\n",
    "\n",
    "To find out if you can use a kernel, you have to be able to use inner product and the way that we can find out is that basically, you're trying to get to a new dimensional space.\n",
    "\n",
    "Up till this point, we have been dealing with an x space, so we would get a z space now for kernels.\n",
    "\n",
    "x = unknown featureset\n",
    "\n",
    "classification --> y= sign(w*x+b)  \n",
    "\n",
    "can we interchange x with z? yes...\n",
    "\n",
    "w * x would return a scalar value, so it doesn't really matter if x is 5 dimensions or 50 dimensions. \n",
    "\n",
    "Therefore, Modifying x space to z space would not have an effect on the classification algorithm.\n",
    "\n",
    "No longer Constraints when you use kernels:\n",
    "- Yi(Xi.w + b ) - 1 >=0, Xi can be interchanged or replaced with Zi\n",
    "- Every interaction is a dot product, so we can get away with kernels.\n",
    "\n",
    "A kernel is not something unique to the Support Vector Machine, it is a similarity function. \n",
    "\n",
    "We can create out machine learning algorithms with kernels.\n",
    "\n",
    "Kernels take 2 inputs and outs them using the inner product. Inner product is a projection of X1 unto X2.\n",
    "\n",
    "We can use a kernel to help us transform our feature space (X space) because every interaction with that feature space is an inner product reaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 30\n",
    "\n",
    "#### Why Kernels?\n",
    "\n",
    "we can utilize a Kernel to help us translate our data to a plausibly infinite number of dimensions in order to find one that has linear separability. We also learned that kernels can let us go out to these dimensions without actually paying the cost for these higher dimensions. Generally, kernels will be defined by something like: k(x, x^) where k = kernel, x^ = x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel function is applied to x and x prime, and will equal the inner product of z and z_prime, where the z values are from the z dimension (our new dimension space).\n",
    "\n",
    "k(x, x^) = z * z^  where * = dot product, z^ = z_prime\n",
    "\n",
    "The z values are the result of some function(x), and these z values are dotted together to give us our kernel function's result.\n",
    "\n",
    "z = a function applied to its x counterpart.\n",
    "\n",
    "z = function(x)\n",
    "\n",
    "z^ = function (z^)\n",
    "\n",
    "dot product or inner product produces a scalar value.\n",
    "\n",
    "We still have yet to cover how this saves us any processing, so let's see an example. We'll start with the polynomial kernel, and compare the requirements of a polynomial kernel to simply taking our current vector and creating a 2nd order polynomial from it.\n",
    "\n",
    "X = [X1, X2] Z = [1, X1, X2, X1 squared, X2 squared, X1X2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel applies the same function both x and x prime, so we'd make the same thing for z prime (x prime to the second order polynomial). From there, the final step is to take the dot product of the two:\n",
    "\n",
    "X = [X1, X2]   Z = [1, X1, X2, X1 squared, X2 squared, X1X2]\n",
    "\n",
    "K (X, X^)   Z^ = [1, X1^, X2^, X1^squared, X2^squared, X1^X2^]\n",
    "\n",
    "k(x, x^) = z * z^ = 1 + X1 X1^ + X2 X2^ + X1squared X1^squared + X2squared X2^squared + X1 X1^ + X2 X2^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all of that work was us manually working through a similar operation that the polynomial kernel is going to do. Luckily for us, our starting dimensions were only two! Now let's consider the polynomial kernel:\n",
    "\n",
    "Can we do the kernel without visiting the z space? yes... by using the polynomial kernel.\n",
    "\n",
    "k(x, x^) = (1 + X * X^) raised to the power of p\n",
    "\n",
    "Notice right away, there are NO Z's mentioned here. This entire kernel is calculated using ONLY the x space! All you need here is to calculate using n number of dimensions and p for the power you want to use. Your equation will look something like:\n",
    "\n",
    "k(x, x^) = (1 + X * X^) raised to the power of p\n",
    "\n",
    "(1 + X1  X1^ + ... Xn Xn^) raised to the power of p\n",
    "\n",
    "If you calculate this all out, your new vectors, which would correspond to the z-space vectors would be something like:\n",
    "\n",
    "[1, X1, X2, _/2 X1,_/2 X2, _/2 X1 X2]    where _/ = square root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, you never need to go out that far. You simply stick with the polynomial kernel, which is going to simply return the dot product for you, without you needing to actually calculate the vectors then take a very large dot product!\n",
    "\n",
    "There are quite a few pre-made kernels, but the only other one I will show here is the Radial Basis Function (RBF) kernel, purely since it's typically the default kernel used, and can take us to a proposed \"infinite\" number of dimensions\n",
    "\n",
    "k(x, x^) = exp (-& ||x - x^|| squared)\n",
    "\n",
    "where & = gamma\n",
    "\n",
    "exp (x) = e raised to power x\n",
    "\n",
    "The value for gamma there is the topic of some possibly future tutorial. So there you have kernels, why you would want to use them, how to use them, and hopefully a decent depiction of how they can allow you to work with larger dimensions without paying the extremely high processing costs! In the next tutorial, we're going to talk about another solution to both non-linear data, as well as to over-fitment issues with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 31\n",
    "\n",
    "**Soft Margin SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you overfit to historical data, it's just likely that in the future data, you won't be right because the future would be different from the past data, it might have noise, so if you fit the past data, you would mess up in the future data.\n",
    "\n",
    "A classifier with violating data in the separating hyperplane is known as a soft margin SVM.\n",
    "\n",
    "There are two major reasons why the soft-margin classifier might be superior.\n",
    "\n",
    "- One reason is your data is not perfectly linearly separable, but is very close and it makes more sense to continue using the default linearly kernel. \n",
    "- The other reason is, even if you are using a kernel, you may wind up with significant over-fitment if you want to use a hard-margin.\n",
    "\n",
    "A classifier having perfectly separated data points in the decision hyperplane is known as a Hard Margin SVM\n",
    "\n",
    "we have here is a \"soft margin\" classifier, which allows for some \"slack\" on the errors that we might get in the optimization process.\n",
    "\n",
    "The value of slack in SVM can best be represented with S >= 0\n",
    "\n",
    "Yi (Xi * W + b) >= 1 - Slack\n",
    "\n",
    "Our new optimization is the above calculation, where slack is greater than or equal to zero. \n",
    "\n",
    "The closer to 0 the slack is, the more \"hard-margin\" we are. The higher the slack, the more soft the margin is. \n",
    "\n",
    "If slack was 0, then we'd have a typical hard-margin classifier. As you might guess, however, we'd like to ideally minimize slack. To do this, we add it to the minimization of the magnitude of vector w:\n",
    "\n",
    "Thus, we actually want to minimize 1/2||w||^2 + (C * The sum of all of the slacks used).With that, we brough in yet another variable, C. \n",
    "\n",
    "C is a multiplier for the \"value\" of how much we want slack to affect the rest of the equation. The lower C, the less important the sum of the slacks is in relation to the magnitude of vector w, and visa versa. In most cases, C will be defaulted to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 32\n",
    "\n",
    "**Kernels, Soft Margin SVM, and Quadratic Programming with Python and CVXOPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python-version of kernels, soft-margin, and solving the quadratic programming problem with CVXOPT\n",
    "\n",
    "You will likely never actually need to use CVXOPT. The library that most people use for the Support Vector Machine optimization is LibSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-236f5f6490b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "             \n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl\n",
    "\n",
    "    def gen_lin_separable_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[0.8, 0.6], [0.6, 0.8]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_non_lin_separable_data():\n",
    "        mean1 = [-1, 2]\n",
    "        mean2 = [1, -1]\n",
    "        mean3 = [4, -4]\n",
    "        mean4 = [-4, 4]\n",
    "        cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 50)\n",
    "        X1 = np.vstack((X1, np.random.multivariate_normal(mean3, cov, 50)))\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 50)\n",
    "        X2 = np.vstack((X2, np.random.multivariate_normal(mean4, cov, 50)))\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_lin_separable_overlap_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[1.5, 1.0], [1.0, 1.5]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def split_train(X1, y1, X2, y2):\n",
    "        X1_train = X1[:90]\n",
    "        y1_train = y1[:90]\n",
    "        X2_train = X2[:90]\n",
    "        y2_train = y2[:90]\n",
    "        X_train = np.vstack((X1_train, X2_train))\n",
    "        y_train = np.hstack((y1_train, y2_train))\n",
    "        return X_train, y_train\n",
    "\n",
    "    def split_test(X1, y1, X2, y2):\n",
    "        X1_test = X1[90:]\n",
    "        y1_test = y1[90:]\n",
    "        X2_test = X2[90:]\n",
    "        y2_test = y2[90:]\n",
    "        X_test = np.vstack((X1_test, X2_test))\n",
    "        y_test = np.hstack((y1_test, y2_test))\n",
    "        return X_test, y_test\n",
    "\n",
    "    def plot_margin(X1_train, X2_train, clf):\n",
    "        def f(x, w, b, c=0):\n",
    "            # given x, return y such that [x,y] in on the line\n",
    "            # w.x + b = c\n",
    "            return (-w[0] * x - b + c) / w[1]\n",
    "\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        # w.x + b = 0\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k\")\n",
    "\n",
    "        # w.x + b = 1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, 1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, 1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        # w.x + b = -1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, -1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, -1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def plot_contour(X1_train, X2_train, clf):\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        X1, X2 = np.meshgrid(np.linspace(-6,6,50), np.linspace(-6,6,50))\n",
    "        X = np.array([[x1, x2] for x1, x2 in zip(np.ravel(X1), np.ravel(X2))])\n",
    "        Z = clf.project(X).reshape(X1.shape)\n",
    "        pl.contour(X1, X2, Z, [0.0], colors='k', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z + 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z - 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def test_linear():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_margin(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_non_linear():\n",
    "        X1, y1, X2, y2 = gen_non_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(polynomial_kernel)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_soft():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_overlap_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(C=1000.1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "        \n",
    "    #test_linear()\n",
    "    #test_non_linear()\n",
    "    #test_soft()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 33\n",
    "\n",
    "**Support Vector Machine Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a binary classifier. This means, at any one time, the SVM optimization is really tasked to separate one group from another. \n",
    "\n",
    "The question is then how we might classify a total of 3 or more groups. \n",
    "\n",
    "Typically, the method is to do what is referred to as \"One Verse Rest\" or (OVR). \n",
    "\n",
    "The idea here is you separate each group from the rest. For example, to classify three separate groups (1, 2, and 3), you would start by separating 1 from 2 and 3. Then you would separate 2 from 1 and 3. Then finally separate 3 from 1 and 2. \n",
    "\n",
    "There are some issues with this, as things like confidence may be different per classification boundary, also the separation boundaries may be slightly flawed since there are almost always going to be more negatives than positives, since you're maybe comparing one group to three others. \n",
    "\n",
    "Assuming a balanced dataset at the start, this would mean every classification boundary is actually unbalanced.\n",
    "\n",
    "Another method is One-vs-One (or OVO). In this case, consider you have three total groups. The way this works is you have a specific boundary that separates 1 from 3 and 1 from 2, and this process repeats for the rest of the classes. In this way, the boundaries may be more balanced.\n",
    "\n",
    "The first parameter is C. This tells you right away that this is a soft-margin classifier. You can adjust C however you like, and you could make C high enough to create a hard-margin classifier. Recall C is used in the soft-margin optimization function for ||w|| i.e. minimize 1/2||w||^2 + (C * The sum of all of the slacks used).\n",
    "\n",
    "The default value for C is just a simple 1, and that really should be fine in most cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have a choice of kernel. The default here is the rbf kernel, but you can also just have a linear kernel, a poly (for polynomial), sigmoid, or even a custom one of your choosing or design.\n",
    "\n",
    "Next, you have the degree value, defaulting to 3, which is just the degree of the polynomial, if you are using the poly value for the kernel.\n",
    "\n",
    "gamma is where you can set the gamma value for the rbf kernel. You should leave this as auto.\n",
    "\n",
    "coef0 allows you to adjust the independent term in your kernel function, but you should also leave this alone most likely, and it is only used in the polynomial and sigmoid kernels.\n",
    "\n",
    "The probability parameter setting may prove useful to you. Recall how an algorithm like K Nearest Neighbors not only has a model accuracy, but also each prediction can have a degree of \"confidence.\" The SVM doesn't inherently have an attribute like this, but you can use this probability parameter to enable a form of one. This is a costly functionality, but may be important enough to you to enable it, otherwise the default is False.\n",
    "\n",
    "Next, we have the shrinking boolean, which is defaulted to True. This has to do with whether or not you want a shrinking heuristic used in your optimization of the SVM, which is used in Sequential Minimal Optimization (SMO). You should leave this True, as it should greatly improve your performance, for very little loss in terms of accuracy in most cases.\n",
    "\n",
    "The tol parameter is a setting for the SVM's tolerance in optimization. Recall that yi(xi.w+b)-1 >= 0. For an SVM to be valid, all values must be greater than or equal to 0, and at least one value on each side needs to be \"equal\" to 0, which will be your support vectors. Since it is highly unlikely that you will actually get values equal perfectly to 0, you set tolerance to allow a bit of wiggle room. The default tol with Scikit-Learn's SVM is 1e-3, which is 0.001.\n",
    "\n",
    "The next important parameter is max_iter, which is where you can set a maximum number of iterations for the quadratic programming problem to cycle through to optimize. The default is -1, which means there is no limit.\n",
    "\n",
    "The decision_function_shape is one-vs-one (ovo) or one-vs-rest (ovr), which is the concept discussed at the beginning of this tutorial.\n",
    "\n",
    "random_state is used for a seed in the probability estimation, if you wanted to specify it. Doing probability on a large dataset would be hard.\n",
    "\n",
    "Aside from the parameters, we also have a few attributes:\n",
    "\n",
    "support_ gives you the index values for the support vectors. support_vectors_ are the actual support vectors. n_support_ will tell you how many support vectors you have, which is useful for comparing to your dataset size to determine if you may have some statistical issues. The last 3 parameters dual_coef_, coef_, and intercept_ will be useful if you plan to graph the SVM, for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
